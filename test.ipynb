{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theory notes:\n",
    "image -> image classifier -> cat\n",
    "\n",
    "Video:\n",
    "-split into images -> classify images\n",
    "-average across prediciton probabilities:\n",
    "-late fusion approach\n",
    "\n",
    "CNNs (convolutional neural network):\n",
    "-filters(kernels)\n",
    "\n",
    "early fusion\n",
    "\n",
    "3D conv neural network\n",
    "- slow fusion\n",
    "- slow computational\n",
    "\n",
    "\n",
    "LSTM (long short term memory) network\n",
    "- best on sequence data\n",
    "- individual frames sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os \n",
    "import math\n",
    "import  numpy as np\n",
    "import datetime as dt\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "from moviepy.editor import *\n",
    "from tensorflow.keras.layers import *\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VISUALIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restrict the randomness\n",
    "seed_constant = 27\n",
    "np.random.seed(seed_constant)\n",
    "random.seed(seed_constant)\n",
    "tf.random.set_seed(seed_constant)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "#reading class names\n",
    "with open('dataset/labels_dict.json', 'r') as f:\n",
    "    js = json.load(f)\n",
    "\n",
    "with open(\"dataset/annotation_dict.json\", \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "all_classes_names = list(js.values())\n",
    "\n",
    "\n",
    "random_range = random.sample(range(len(all_classes_names)-1),10)\n",
    "\n",
    "\n",
    "\n",
    "#iterating through all generated random values\n",
    "for counter, random_index in enumerate(random_range,1):\n",
    "    #selcet class name\n",
    "    selected_class_name = all_classes_names[random_index]\n",
    "    \n",
    "    video_file_names_list = []\n",
    "\n",
    "    #list of all video files with class\n",
    "    for el in annotations.keys():\n",
    "        \n",
    "        if annotations[el] == random_index:\n",
    "            video_file_names_list.append(el + \".mp4\")\n",
    "    \n",
    "   \n",
    "    #random selection of video from the class\n",
    "    selected_video_file = random.choice(video_file_names_list)\n",
    "   \n",
    "\n",
    "    #read video\n",
    "    \n",
    "    video = cv2.VideoCapture(\"dataset/examples/\" + selected_video_file)\n",
    "    \n",
    "    #read firstr frame\n",
    "    _, frame = video.read()\n",
    "\n",
    "    #release video(only read onw frame)\n",
    "    video.release()\n",
    "\n",
    "    #convert to rgb\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    #write text on the video frame\n",
    "    cv2.putText(rgb_frame, selected_class_name, (30, 30), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 2)\n",
    "    #display the frame\n",
    "    plt.subplot(5,4,counter)\n",
    "    plt.imshow(rgb_frame)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCCESING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resize \n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64,64\n",
    "\n",
    "#number of frames fed into the model\n",
    "SEQUENCE_LENGTH = 10\n",
    "\n",
    "#dataset dir\n",
    "DATASET_DIR = \"dataset/examples\"\n",
    "\n",
    "#classes\n",
    "CLASS_LIST =  all_classes_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to extract, resize and normalize frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_extraction(video_path):\n",
    "\n",
    "    #list to store video frames\n",
    "    frames_list = []\n",
    "\n",
    "    #read the video\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    #iterate trough video frames\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "\n",
    "\n",
    "        #reading the frame \n",
    "        success,frame = video.read()\n",
    "\n",
    "        #if the frame is not read correctly exit the loop\n",
    "        if not success:\n",
    "            break\n",
    "            \n",
    "    \n",
    "\n",
    "        #resize the frame\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "\n",
    "        \n",
    "\n",
    "        #normalize the frame (good for neural networks)\n",
    "        normalized_frame = resized_frame / 255.0\n",
    "\n",
    "        \n",
    "\n",
    "        #append the normalized frame\n",
    "        frames_list.append(normalized_frame)\n",
    "       \n",
    "\n",
    "    #release the video\n",
    "    video.release()\n",
    "\n",
    "    \n",
    "    return frames_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset creation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    features = []\n",
    "    labels = []\n",
    "    video_files_paths = []\n",
    "    \n",
    "\n",
    "    #iterate through all the classes\n",
    "    for class_index, class_name in enumerate(CLASS_LIST):\n",
    "\n",
    "        #display the name of the class whose data is being extracted\n",
    "        print(f\"Extracting Data of Class: {class_name}\")\n",
    "\n",
    "        #get the list of video files present in the specific class name directory\n",
    "        files_list = os.listdir(os.path.join(DATASET_DIR))\n",
    "\n",
    "        #iterate through all the files present in the files list\n",
    "        for file_name in files_list:\n",
    "            \n",
    "            #check if list has the key\n",
    "            \n",
    "            if file_name[:-4] in annotations.keys() and not(file_name[2] == \"3\"):\n",
    "                #check if file has the class\n",
    "                if annotations[file_name[:-4]] == class_index:\n",
    "\n",
    "                    #get the complete video path\n",
    "                    video_file_path = os.path.join(DATASET_DIR, file_name)\n",
    "\n",
    "                    #get the frames from the video\n",
    "                    frames = frame_extraction(video_file_path)\n",
    "                    \n",
    "                    \n",
    "                    #igonre the files with less than 20 frames\n",
    "                    if len(frames) == SEQUENCE_LENGTH:\n",
    "                       \n",
    "                        \n",
    "                        #append the data to the correct list\n",
    "                        features.append(frames)\n",
    "                        labels.append(class_index)\n",
    "                        video_files_paths.append(video_file_path)\n",
    "\n",
    "  \n",
    "\n",
    "    return features, labels, video_files_paths\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize the function create_dataset - getting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataset\n",
    "features, labels, video_files_paths = create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels to one_hot_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using keras to_categorical\n",
    "labels = tf.keras.utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split intoo train set(75%) and test set(25%)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement convLSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " #construct the model \n",
    "def build_model():\n",
    "   \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(ConvLSTM2D(filters= 4, kernel_size=(3,3), activation='tanh', data_format=\"channels_last\",recurrent_dropout=0.2, return_sequences=True,input_shape=(SEQUENCE_LENGTH, IMAGE_WIDTH, IMAGE_HEIGHT, 3)))\n",
    "\n",
    "    model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format=\"channels_last\"))\n",
    "\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "\n",
    "\n",
    "    model.add(ConvLSTM2D(filters= 8, kernel_size=(3,3), activation='tanh', data_format=\"channels_last\",recurrent_dropout=0.2, return_sequences=True,input_shape=(SEQUENCE_LENGTH, IMAGE_WIDTH, IMAGE_HEIGHT, 3)))\n",
    "\n",
    "    model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format=\"channels_last\"))\n",
    "\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "\n",
    "\n",
    "    model.add(ConvLSTM2D(filters= 14, kernel_size=(3,3), activation='tanh', data_format=\"channels_last\",recurrent_dropout=0.2, return_sequences=True,input_shape=(SEQUENCE_LENGTH, IMAGE_WIDTH, IMAGE_HEIGHT, 3)))\n",
    "\n",
    "    model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format=\"channels_last\"))\n",
    "\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "\n",
    "\n",
    "    model.add(ConvLSTM2D(filters= 16, kernel_size=(3,3), activation='tanh', data_format=\"channels_last\",recurrent_dropout=0.2, return_sequences=True,input_shape=(SEQUENCE_LENGTH, IMAGE_WIDTH, IMAGE_HEIGHT, 3)))\n",
    "\n",
    "    model.add(MaxPooling3D(pool_size=(1,2,2), padding='same', data_format=\"channels_last\"))\n",
    "\n",
    "    model.add(TimeDistributed(Dropout(0.2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    #softmax gives you probabilities of each class\n",
    "    model.add(Dense(len(CLASS_LIST), activation='softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create instance of Early stopping callback\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min', restore_best_weights = True)\n",
    "\n",
    "#Compile the model and specify loss function, optimizer and metrics values to the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"finished compiling\")\n",
    "\n",
    "#train the model\n",
    "history = model.fit(train_features, train_labels, epochs = 50, batch_size = 2, validation_split=0.2, callbacks = [early_stopping_callback])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "activity_recognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
